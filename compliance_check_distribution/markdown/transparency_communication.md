| Task Name                      | Compliance Check                    | Compliance description                                                                                                                     |
|--------------------------------|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|
| Interpretability Analysis      | Model Explainability Implementation | Verify if the code uses SHAP, LIME, or feature importance scores to explain model predictions.                                             |
| Explainability Features        | Model Interpretability Tools        | Check if the code incorporates tools like SHAP, LIME, or attention mechanisms to explain model decisions.                                  |
| Performance Metric Calculation | Evaluation Metric Computation       | Check if the code computes relevant metrics such as accuracy, precision, recall, F1-score for classification, or RMSE, MAE for regression. |
| Uncertainty Estimation         | Confidence & Uncertainty Measures   | Verify if the code includes methods such as Bayesian inference or dropout-based uncertainty estimation.                                    |
| Bias and Fairness Testing      | Bias Detection & Mitigation         | Assess whether the code includes techniques to detect and mitigate model biases against specific groups.                                   |
| API Integration                | Model API Development               | Ensure the code builds REST or gRPC APIs for serving predictions.                                                                          |
| Logging and Debugging          | Prediction & Failure Logging        | Ensure the code maintains logs of predictions, failures, and user feedback for debugging.                                                  |
