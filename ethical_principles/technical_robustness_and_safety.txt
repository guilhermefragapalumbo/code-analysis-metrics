Technical robustness and safety: A crucial component of achieving Trustworthy AI is technical robustness, which is closely linked to the principle of prevention of harm. Technical robustness requires that AI systems be developed with a preventative approach to risks and in a manner such that they reliably behave as intended while minimising unintentional and unexpected  harm, and preventing unacceptable harm. This should also apply to potential changes in their operating environment or the presence of other agents (human and artificial) that may interact with the system in an  adversarial manner. In addition, the physical and mental integrity of humans should be ensured.
Resilience to attack and security: AI systems, like all software systems, should be protected against vulnerabilities that can allow them to be exploited by adversaries, e.g. hacking. Attacks may target the data (data poisoning), the  model (model leakage) or the underlying infrastructure, both software and hardware. If an AI system is attacked,  e.g. in adversarial attacks, the data as well as system behaviour can be changed, leading the system to make  different decisions, or causing it to shut down altogether. Systems and data can also become corrupted by malicious  intention or by exposure to unexpected situations. Insufficient security processes can also result in erroneous  decisions or even physical harm. For AI systems to be considered secure, possible unintended applications of the  AI system (e.g. dual-use applications) and potential abuse of the system by malicious actors should be taken into  account, and steps should be taken to prevent and mitigate these.
Fallback plan and general safety: AI systems should have safeguards that enable a fallback plan in case of problems. This can mean that AI systems switch from a statistical to rule-based procedure, or that they ask for a human operator before continuing their action. It must be ensured that the system will do what it is supposed to do  without harming living beings or the environment. This includes the minimisation of unintended consequences and  errors. In addition, processes to clarify and assess potential risks associated with the use of AI systems, across  various application areas, should be established. The level of safety measures required depends on the magnitude  of the risk posed by an AI system, which in turn depends on the system’s capabilities. Where it can be foreseen that  the development process or the system itself will pose particularly high risks, it is crucial for safety measures to be  developed and tested proactively.
Accuracy: Accuracy pertains to an AI system’s ability to make correct judgements, for example to correctly classify information into the proper categories, or its ability to make correct predictions, recommendations, or decisions  based on data or models. An explicit and well-formed development and evaluation process can support, mitigate  and correct unintended risks from inaccurate predictions. When occasional inaccurate predictions cannot be  avoided, it is important that the system can indicate how likely these errors are. A high level of accuracy is especially  crucial in situations where the AI system directly affects human lives.
Reliability and Reproducibility: It is critical that the results of AI systems are reproducible, as well as reliable. A reliable AI system is one that works properly with a range of inputs and in a range of situations. This is needed to  scrutinise an AI system and to prevent unintended harms. Reproducibility describes whether an AI experiment  exhibits the same behaviour when repeated under the same conditions. This enables scientists and policy makers to  accurately describe what AI systems do. Replication files can facilitate the process of testing and reproducing  behaviours.